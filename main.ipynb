{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Imports, Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timezone\n",
    "import threading\n",
    "import logging\n",
    "import colorlog\n",
    "import warnings\n",
    "\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging levels and suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress Py4J and Spark logs\n",
    "logging.getLogger('py4j').setLevel(logging.ERROR)\n",
    "logging.getLogger('org.apache.spark').setLevel(logging.ERROR)\n",
    "\n",
    "# Configure colored logging\n",
    "handler = colorlog.StreamHandler()\n",
    "handler.setFormatter(colorlog.ColoredFormatter(\n",
    "    \"%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    log_colors={\n",
    "        'DEBUG': 'cyan',\n",
    "        'INFO': 'green',\n",
    "        'WARNING': 'yellow',\n",
    "        'ERROR': 'red',\n",
    "        'CRITICAL': 'bold_red',\n",
    "    }\n",
    "))\n",
    "logger = logging.getLogger('KafkaLogger')\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Kafka Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_config = {\n",
    "    \"bootstrap_servers\": ['77.81.230.104:9092'],\n",
    "    \"username\": 'admin',\n",
    "    \"password\": 'VawEzo1ikLtrA8Ug8THa',\n",
    "    \"security_protocol\": 'SASL_PLAINTEXT',\n",
    "    \"sasl_mechanism\": 'PLAIN'\n",
    "}\n",
    "\n",
    "# Common configurations for producers and consumers\n",
    "common_consumer_config = {\n",
    "    'bootstrap_servers': kafka_config['bootstrap_servers'],\n",
    "    'security_protocol': kafka_config['security_protocol'],\n",
    "    'sasl_mechanism': kafka_config['sasl_mechanism'],\n",
    "    'sasl_plain_username': kafka_config['username'],\n",
    "    'sasl_plain_password': kafka_config['password'],\n",
    "    'value_deserializer': lambda m: json.loads(m.decode('utf-8')),\n",
    "    'auto_offset_reset': 'earliest',\n",
    "    'enable_auto_commit': True,\n",
    "}\n",
    "\n",
    "common_producer_config = {\n",
    "    'bootstrap_servers': kafka_config['bootstrap_servers'],\n",
    "    'security_protocol': kafka_config['security_protocol'],\n",
    "    'sasl_mechanism': kafka_config['sasl_mechanism'],\n",
    "    'sasl_plain_username': kafka_config['username'],\n",
    "    'sasl_plain_password': kafka_config['password'],\n",
    "    'value_serializer': lambda v: json.dumps(v).encode('utf-8'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique identifier\n",
    "unique_id = 'eod_goit_de_hw_06'\n",
    "\n",
    "# Topic names\n",
    "topic_building_sensors = f'building_sensors_{unique_id}'\n",
    "topic_alerts = f'alerts_{unique_id}'\n",
    "\n",
    "# Alerts conditions file\n",
    "alerts_conditions_file = 'alerts_conditions.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create Kafka Topics if They Do Not Exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 20:07:59 - KafkaLogger - INFO - Creating KafkaAdminClient...\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - KafkaAdminClient created successfully.\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - Successfully retrieved list of existing topics.\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - Topic 'building_sensors_eod_goit_de_hw_06' already exists, skipping creation.\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - Topic 'alerts_eod_goit_de_hw_06' already exists, skipping creation.\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - All topics already exist, no need to create new ones.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define topic names\n",
    "topics = [topic_building_sensors, topic_alerts]\n",
    "\n",
    "# Create KafkaAdminClient\n",
    "try:\n",
    "    logger.info(\"Creating KafkaAdminClient...\")\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "        security_protocol=kafka_config['security_protocol'],\n",
    "        sasl_mechanism=kafka_config['sasl_mechanism'],\n",
    "        sasl_plain_username=kafka_config['username'],\n",
    "        sasl_plain_password=kafka_config['password'],\n",
    "        client_id='admin_client'\n",
    "    )\n",
    "    logger.info(\"KafkaAdminClient created successfully.\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error creating KafkaAdminClient: {e}\")\n",
    "    raise\n",
    "\n",
    "# Get list of existing topics\n",
    "try:\n",
    "    existing_topics = admin_client.list_topics()\n",
    "    logger.info(\"Successfully retrieved list of existing topics.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error retrieving list of topics: {e}\")\n",
    "    raise\n",
    "\n",
    "# Create only topics that do not exist\n",
    "topic_list = []\n",
    "for topic_name in topics:\n",
    "    if topic_name not in existing_topics:\n",
    "        topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1)\n",
    "        topic_list.append(topic)\n",
    "    else:\n",
    "        logger.info(f\"Topic '{topic_name}' already exists, skipping creation.\")\n",
    "\n",
    "if topic_list:\n",
    "    try:\n",
    "        logger.info(\"Attempting to create new topics...\")\n",
    "        admin_client.create_topics(new_topics=topic_list, validate_only=False)\n",
    "        logger.info(\"Topics created successfully:\")\n",
    "        for topic in topic_list:\n",
    "            logger.info(f\"- {topic.name}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating topics: {e}\")\n",
    "else:\n",
    "    logger.info(\"All topics already exist, no need to create new ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Sensor Simulation Function with Extended Data Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate temperature values\n",
    "def generate_temperature():\n",
    "    if random.random() < 0.8:  # 80% of the time, generate realistic data\n",
    "        return random.uniform(-50, 50)\n",
    "    else:  # 20% of the time, generate extreme values\n",
    "        return random.uniform(-300, 300)\n",
    "\n",
    "# Sensor simulation function\n",
    "def sensor_simulation(sensor_id, topic_name):\n",
    "    producer = KafkaProducer(**common_producer_config)\n",
    "    try:\n",
    "        while True:\n",
    "            # Generate temperature and humidity\n",
    "            temperature = generate_temperature()\n",
    "            humidity = random.uniform(0, 100)\n",
    "            timestamp = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "            # Create message\n",
    "            data = {\n",
    "                \"sensor_id\": sensor_id,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"temperature\": round(temperature, 2),\n",
    "                \"humidity\": round(humidity, 2),\n",
    "            }\n",
    "\n",
    "            # Send to Kafka\n",
    "            producer.send(topic_name, value=data)\n",
    "            logger.info(f\"Sent message from sensor {sensor_id}: {data}\")\n",
    "\n",
    "            time.sleep(5)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(f\"Sensor simulation {sensor_id} stopped.\")\n",
    "    finally:\n",
    "        producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - Starting 3 sensor simulations with IDs: [4373, 4351, 7337]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start simulation for multiple sensors\n",
    "num_sensors = 3\n",
    "sensor_ids = random.sample(range(1000, 9999), num_sensors)\n",
    "\n",
    "logger.info(f\"Starting {num_sensors} sensor simulations with IDs: {sensor_ids}\")\n",
    "\n",
    "sensor_threads = []\n",
    "for sensor_id in sensor_ids:\n",
    "    sensor_thread = threading.Thread(target=sensor_simulation, args=(sensor_id, topic_building_sensors))\n",
    "    sensor_thread.daemon = True\n",
    "    sensor_thread.start()\n",
    "    sensor_threads.append(sensor_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:00.876923+00:00', 'temperature': 4.22, 'humidity': 92.63}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:00.881755+00:00', 'temperature': -17.88, 'humidity': 49.45}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:00 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:00.881709+00:00', 'temperature': 10.41, 'humidity': 82.49}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:05 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:05.922389+00:00', 'temperature': -15.85, 'humidity': 27.8}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:05 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:05.932681+00:00', 'temperature': -11.19, 'humidity': 24.62}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:05 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:05.933871+00:00', 'temperature': -36.5, 'humidity': 34.93}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:10 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:10.923899+00:00', 'temperature': 13.9, 'humidity': 87.93}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:10 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:10.934423+00:00', 'temperature': -160.55, 'humidity': 15.79}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:10 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:10.936508+00:00', 'temperature': 284.43, 'humidity': 37.42}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:15 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:15.925935+00:00', 'temperature': -4.39, 'humidity': 0.86}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:15 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:15.936326+00:00', 'temperature': -38.56, 'humidity': 47.58}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:15 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:15.939322+00:00', 'temperature': 3.23, 'humidity': 89.06}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:20 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:20.929578+00:00', 'temperature': 17.64, 'humidity': 30.3}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:20 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:20.946769+00:00', 'temperature': 39.01, 'humidity': 95.32}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:20 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:20.949999+00:00', 'temperature': -57.99, 'humidity': 82.78}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:25 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:25.937545+00:00', 'temperature': 22.16, 'humidity': 11.58}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:25 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:25.960152+00:00', 'temperature': 4.34, 'humidity': 44.63}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:25 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:25.968295+00:00', 'temperature': 28.91, 'humidity': 47.66}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:30 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:30.941022+00:00', 'temperature': -21.25, 'humidity': 50.32}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:30 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:30.965293+00:00', 'temperature': 275.69, 'humidity': 64.64}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:30 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:30.970085+00:00', 'temperature': 41.31, 'humidity': 56.21}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:35 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:35.947522+00:00', 'temperature': -2.38, 'humidity': 44.76}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:35 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:35.975457+00:00', 'temperature': -49.45, 'humidity': 99.97}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:35 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:35.982276+00:00', 'temperature': 25.44, 'humidity': 42.58}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:40 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:40.961685+00:00', 'temperature': -35.31, 'humidity': 82.22}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:40 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:40.987526+00:00', 'temperature': 25.31, 'humidity': 12.69}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:40 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:40.994138+00:00', 'temperature': -35.96, 'humidity': 33.4}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:45 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:45.975032+00:00', 'temperature': -13.72, 'humidity': 83.42}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:45 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:45.996344+00:00', 'temperature': -184.97, 'humidity': 11.51}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:46 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:46.003471+00:00', 'temperature': -40.69, 'humidity': 75.47}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:50 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:50.984588+00:00', 'temperature': 29.46, 'humidity': 61.91}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:51 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:51.014222+00:00', 'temperature': 45.18, 'humidity': 6.85}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:51 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:51.034617+00:00', 'temperature': 22.56, 'humidity': 37.27}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:55 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:08:55.990159+00:00', 'temperature': 49.6, 'humidity': 75.31}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:56 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:08:56.024212+00:00', 'temperature': -5.93, 'humidity': 24.73}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:56 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:08:56.041066+00:00', 'temperature': 10.04, 'humidity': 69.3}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:00 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:00.994015+00:00', 'temperature': 2.68, 'humidity': 39.29}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:01 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:01.032652+00:00', 'temperature': -2.13, 'humidity': 55.35}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:01 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:01.045776+00:00', 'temperature': -27.35, 'humidity': 12.51}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:05 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:05.997643+00:00', 'temperature': 27.5, 'humidity': 85.96}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:06 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:06.034878+00:00', 'temperature': 34.81, 'humidity': 39.96}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:06 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:06.052043+00:00', 'temperature': -43.94, 'humidity': 75.21}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:11 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:11.006632+00:00', 'temperature': -42.23, 'humidity': 2.13}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:11 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:11.050193+00:00', 'temperature': -4.52, 'humidity': 63.19}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:11 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:11.060266+00:00', 'temperature': 0.85, 'humidity': 58.53}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:16 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:16.018034+00:00', 'temperature': 18.87, 'humidity': 42.79}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:16 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:16.069618+00:00', 'temperature': 30.64, 'humidity': 51.68}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:16 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:16.086979+00:00', 'temperature': 5.88, 'humidity': 12.35}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:21 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:21.024402+00:00', 'temperature': 10.4, 'humidity': 21.98}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:21 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:21.076385+00:00', 'temperature': 31.6, 'humidity': 4.83}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:21 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:21.096438+00:00', 'temperature': 41.62, 'humidity': 74.62}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:26 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:26.035467+00:00', 'temperature': -22.02, 'humidity': 33.08}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:26 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:26.089252+00:00', 'temperature': 39.19, 'humidity': 70.29}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:26 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:26.108168+00:00', 'temperature': 45.58, 'humidity': 77.27}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:31 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:31.046043+00:00', 'temperature': -34.65, 'humidity': 4.04}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:31 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:31.103137+00:00', 'temperature': 40.04, 'humidity': 45.39}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:31 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:31.116715+00:00', 'temperature': -24.92, 'humidity': 55.98}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:36 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:36.060652+00:00', 'temperature': -13.46, 'humidity': 95.02}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:36 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:36.114411+00:00', 'temperature': -193.33, 'humidity': 4.1}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:36 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:36.129089+00:00', 'temperature': 21.93, 'humidity': 47.8}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:41 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:41.092804+00:00', 'temperature': 30.73, 'humidity': 4.8}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:41 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:41.123556+00:00', 'temperature': 172.2, 'humidity': 45.86}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:41 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:41.142718+00:00', 'temperature': 20.73, 'humidity': 68.98}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:46 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:46.101184+00:00', 'temperature': -12.87, 'humidity': 95.26}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:46 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:46.152643+00:00', 'temperature': 39.89, 'humidity': 38.43}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:46 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:46.157272+00:00', 'temperature': -30.17, 'humidity': 97.82}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:51 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:51.113154+00:00', 'temperature': -39.23, 'humidity': 47.27}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:51 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:51.172907+00:00', 'temperature': 90.58, 'humidity': 17.09}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:51 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:51.178876+00:00', 'temperature': -2.72, 'humidity': 25.37}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:56 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:09:56.114684+00:00', 'temperature': 6.36, 'humidity': 9.53}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:56 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:09:56.174986+00:00', 'temperature': 28.05, 'humidity': 93.68}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:56 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:09:56.180958+00:00', 'temperature': -23.83, 'humidity': 42.81}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:01 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:01.118553+00:00', 'temperature': 5.19, 'humidity': 88.64}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:01 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:01.178919+00:00', 'temperature': 248.77, 'humidity': 11.47}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:01 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:01.183821+00:00', 'temperature': 217.39, 'humidity': 79.46}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:06 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:06.123300+00:00', 'temperature': 44.21, 'humidity': 88.75}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:06 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:06.183283+00:00', 'temperature': 34.54, 'humidity': 19.38}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:06 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:06.186880+00:00', 'temperature': -19.29, 'humidity': 90.83}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:11 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:11.125590+00:00', 'temperature': 39.43, 'humidity': 60.67}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:11 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:11.187624+00:00', 'temperature': 1.82, 'humidity': 52.69}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:11 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:11.191085+00:00', 'temperature': -24.2, 'humidity': 80.3}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:16 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:16.129766+00:00', 'temperature': -254.16, 'humidity': 71.02}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:16 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:16.190934+00:00', 'temperature': -24.54, 'humidity': 88.17}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:16 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:16.194221+00:00', 'temperature': 32.97, 'humidity': 90.49}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:21 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:21.132412+00:00', 'temperature': 5.96, 'humidity': 21.49}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:21 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:21.194621+00:00', 'temperature': 48.17, 'humidity': 16.84}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:21 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:21.196778+00:00', 'temperature': -39.84, 'humidity': 92.5}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:26 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:26.136641+00:00', 'temperature': 18.96, 'humidity': 56.48}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:26 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:26.198487+00:00', 'temperature': 20.84, 'humidity': 17.3}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:26 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:26.200332+00:00', 'temperature': 24.07, 'humidity': 46.83}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:31 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:31.138673+00:00', 'temperature': 14.35, 'humidity': 81.66}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:31 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:31.202101+00:00', 'temperature': 37.8, 'humidity': 73.83}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:31 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:31.203054+00:00', 'temperature': -42.39, 'humidity': 29.52}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:36 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:36.142093+00:00', 'temperature': -47.59, 'humidity': 73.79}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:36 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:36.207126+00:00', 'temperature': -4.98, 'humidity': 33.68}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:36 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:36.209467+00:00', 'temperature': 17.44, 'humidity': 79.81}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:41 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:41.146710+00:00', 'temperature': 26.95, 'humidity': 60.21}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:41 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:41.211869+00:00', 'temperature': 45.88, 'humidity': 71.49}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:41 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:41.214548+00:00', 'temperature': -40.77, 'humidity': 41.53}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:46 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:46.148843+00:00', 'temperature': 2.64, 'humidity': 99.56}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:46 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:46.217698+00:00', 'temperature': 12.09, 'humidity': 12.31}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:46 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:46.220196+00:00', 'temperature': 25.91, 'humidity': 24.46}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:51 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:51.150837+00:00', 'temperature': -19.17, 'humidity': 53.53}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:51 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:51.223460+00:00', 'temperature': 35.83, 'humidity': 27.23}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:51 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:51.227150+00:00', 'temperature': -26.8, 'humidity': 60.55}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:56 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:10:56.153875+00:00', 'temperature': -25.98, 'humidity': 82.54}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:56 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:10:56.226117+00:00', 'temperature': -7.17, 'humidity': 92.47}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:10:56 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:10:56.230929+00:00', 'temperature': -17.77, 'humidity': 1.33}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:01 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:01.156176+00:00', 'temperature': 42.38, 'humidity': 64.76}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:01 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:01.230782+00:00', 'temperature': 19.81, 'humidity': 64.62}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:01 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:01.233828+00:00', 'temperature': 23.96, 'humidity': 28.79}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:06 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:06.160740+00:00', 'temperature': -26.19, 'humidity': 82.86}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:06 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:06.234299+00:00', 'temperature': -167.39, 'humidity': 33.42}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:06 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:06.236976+00:00', 'temperature': -4.55, 'humidity': 54.1}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:11 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:11.163376+00:00', 'temperature': -16.77, 'humidity': 85.88}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:11 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:11.237011+00:00', 'temperature': -26.52, 'humidity': 30.01}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:11 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:11.241709+00:00', 'temperature': -185.83, 'humidity': 4.67}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:16 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:16.167076+00:00', 'temperature': -29.93, 'humidity': 35.77}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:16 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:16.241274+00:00', 'temperature': -28.3, 'humidity': 3.78}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:16 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:16.244884+00:00', 'temperature': -46.76, 'humidity': 55.43}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:21 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:21.170881+00:00', 'temperature': -46.11, 'humidity': 67.74}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:21 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:21.245820+00:00', 'temperature': 48.86, 'humidity': 22.18}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:21 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:21.247924+00:00', 'temperature': 21.82, 'humidity': 28.44}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:26 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:26.171919+00:00', 'temperature': 266.45, 'humidity': 85.81}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:26 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:26.247571+00:00', 'temperature': 11.85, 'humidity': 17.06}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:26 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:26.251772+00:00', 'temperature': -18.12, 'humidity': 2.53}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:31 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:31.174371+00:00', 'temperature': 36.3, 'humidity': 5.13}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:31 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:31.253190+00:00', 'temperature': 5.67, 'humidity': 64.04}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:31 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:31.258033+00:00', 'temperature': 115.53, 'humidity': 28.15}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:36 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:36.177709+00:00', 'temperature': 40.54, 'humidity': 0.63}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:36 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:36.258117+00:00', 'temperature': -1.77, 'humidity': 31.25}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:36 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:36.260656+00:00', 'temperature': -33.13, 'humidity': 53.52}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:41 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:41.179060+00:00', 'temperature': -43.01, 'humidity': 82.3}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:41 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:41.259986+00:00', 'temperature': -18.03, 'humidity': 98.16}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:41 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:41.263551+00:00', 'temperature': 10.9, 'humidity': 35.55}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:46 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:46.180938+00:00', 'temperature': 12.32, 'humidity': 82.76}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:46 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:46.263340+00:00', 'temperature': 29.89, 'humidity': 74.81}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:46 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:46.266169+00:00', 'temperature': -1.34, 'humidity': 1.33}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:51 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:51.183021+00:00', 'temperature': -264.64, 'humidity': 85.07}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:51 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:51.268044+00:00', 'temperature': -144.79, 'humidity': 90.83}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:51 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:51.270607+00:00', 'temperature': -154.14, 'humidity': 77.6}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:56 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:11:56.186491+00:00', 'temperature': 42.54, 'humidity': 11.74}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:56 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:11:56.269427+00:00', 'temperature': -79.14, 'humidity': 23.18}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:11:56 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:11:56.272974+00:00', 'temperature': -44.12, 'humidity': 91.26}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:01 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:01.188449+00:00', 'temperature': 42.11, 'humidity': 10.09}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:01 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:01.272518+00:00', 'temperature': 12.81, 'humidity': 28.48}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:01 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:01.275172+00:00', 'temperature': 28.15, 'humidity': 5.55}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:06 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:06.191414+00:00', 'temperature': 1.26, 'humidity': 54.92}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:06 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:06.275024+00:00', 'temperature': 11.58, 'humidity': 27.26}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:06 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:06.278681+00:00', 'temperature': 2.45, 'humidity': 89.66}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:11 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:11.193043+00:00', 'temperature': -1.26, 'humidity': 84.8}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:11 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:11.281384+00:00', 'temperature': -129.61, 'humidity': 37.15}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:11 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:11.284097+00:00', 'temperature': -135.19, 'humidity': 8.58}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:16 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:16.196459+00:00', 'temperature': -87.84, 'humidity': 40.01}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:16 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:16.288801+00:00', 'temperature': 283.04, 'humidity': 21.98}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:16 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:16.291764+00:00', 'temperature': -0.58, 'humidity': 88.55}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:21 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:21.197971+00:00', 'temperature': 12.28, 'humidity': 62.45}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:21 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:21.295395+00:00', 'temperature': 24.15, 'humidity': 80.04}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:21 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:21.297478+00:00', 'temperature': -33.59, 'humidity': 39.97}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:26 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:26.201089+00:00', 'temperature': 45.17, 'humidity': 24.36}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:26 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:26.301216+00:00', 'temperature': 46.56, 'humidity': 0.75}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:26 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:26.305280+00:00', 'temperature': 206.2, 'humidity': 62.38}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:31 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:31.202418+00:00', 'temperature': 49.06, 'humidity': 42.41}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:31 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:31.306272+00:00', 'temperature': 27.22, 'humidity': 36.48}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:31 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:31.310818+00:00', 'temperature': -28.74, 'humidity': 79.27}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:36 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:36.206565+00:00', 'temperature': -29.96, 'humidity': 33.17}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:36 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:36.311770+00:00', 'temperature': 42.22, 'humidity': 77.74}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:36 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:36.312905+00:00', 'temperature': 43.88, 'humidity': 96.81}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:41 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:41.210849+00:00', 'temperature': 5.41, 'humidity': 32.46}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:41 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:41.318496+00:00', 'temperature': -45.23, 'humidity': 74.7}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:41 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:41.319864+00:00', 'temperature': -3.35, 'humidity': 9.51}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:46 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:46.215574+00:00', 'temperature': -46.21, 'humidity': 80.95}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:46 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:46.322002+00:00', 'temperature': 299.5, 'humidity': 46.26}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:46 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:46.324854+00:00', 'temperature': -9.92, 'humidity': 27.84}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:51 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:51.217439+00:00', 'temperature': 15.19, 'humidity': 78.95}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:51 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:51.326813+00:00', 'temperature': 23.24, 'humidity': 40.3}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:51 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:51.327666+00:00', 'temperature': -43.95, 'humidity': 94.94}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:56 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:12:56.218766+00:00', 'temperature': 33.45, 'humidity': 69.35}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:56 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:12:56.329041+00:00', 'temperature': 28.37, 'humidity': 24.4}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:12:56 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:12:56.331782+00:00', 'temperature': 23.19, 'humidity': 0.88}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:01 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:01.220035+00:00', 'temperature': 29.54, 'humidity': 12.33}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:01 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:01.332002+00:00', 'temperature': 72.25, 'humidity': 59.08}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:01 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:01.334841+00:00', 'temperature': 34.83, 'humidity': 83.7}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:06 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:06.224067+00:00', 'temperature': -42.5, 'humidity': 37.36}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:06 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:06.335111+00:00', 'temperature': 21.19, 'humidity': 80.44}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:06 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:06.338358+00:00', 'temperature': 35.3, 'humidity': 9.55}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:11 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:11.227569+00:00', 'temperature': -33.13, 'humidity': 5.7}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:11 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:11.339055+00:00', 'temperature': 196.2, 'humidity': 81.39}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:11 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:11.341438+00:00', 'temperature': -48.03, 'humidity': 49.79}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:16 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:16.231032+00:00', 'temperature': 43.77, 'humidity': 3.76}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:16 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:16.340933+00:00', 'temperature': 28.52, 'humidity': 98.02}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:16 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:16.343787+00:00', 'temperature': -36.5, 'humidity': 75.19}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:21 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:21.233817+00:00', 'temperature': 37.12, 'humidity': 27.4}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:21 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:21.343158+00:00', 'temperature': 24.09, 'humidity': 85.51}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:21 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:21.347469+00:00', 'temperature': 48.96, 'humidity': 29.56}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:26 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:26.237369+00:00', 'temperature': -1.09, 'humidity': 30.55}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:26 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:26.348129+00:00', 'temperature': -31.64, 'humidity': 50.92}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:26 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:26.351776+00:00', 'temperature': 11.52, 'humidity': 63.89}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:31 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:31.238705+00:00', 'temperature': 2.63, 'humidity': 29.23}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:31 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:31.349494+00:00', 'temperature': 23.02, 'humidity': 7.54}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:31 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:31.353282+00:00', 'temperature': -16.59, 'humidity': 66.41}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:36 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:36.241961+00:00', 'temperature': -0.18, 'humidity': 33.57}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:36 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:36.351672+00:00', 'temperature': 44.77, 'humidity': 69.74}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:36 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:36.354786+00:00', 'temperature': -1.27, 'humidity': 89.14}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:41 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:41.243612+00:00', 'temperature': 8.88, 'humidity': 88.33}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:41 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:41.354057+00:00', 'temperature': 176.52, 'humidity': 57.98}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:41 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:41.356726+00:00', 'temperature': -16.7, 'humidity': 69.06}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:46 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:46.247161+00:00', 'temperature': 18.9, 'humidity': 91.6}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:46 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:46.355031+00:00', 'temperature': 37.86, 'humidity': 11.02}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:46 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:46.358768+00:00', 'temperature': -1.89, 'humidity': 6.14}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:51 - KafkaLogger - INFO - Sent message from sensor 4373: {'sensor_id': 4373, 'timestamp': '2024-11-23T19:13:51.249709+00:00', 'temperature': -281.76, 'humidity': 29.36}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:51 - KafkaLogger - INFO - Sent message from sensor 7337: {'sensor_id': 7337, 'timestamp': '2024-11-23T19:13:51.357331+00:00', 'temperature': 66.6, 'humidity': 35.57}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:13:51 - KafkaLogger - INFO - Sent message from sensor 4351: {'sensor_id': 4351, 'timestamp': '2024-11-23T19:13:51.360591+00:00', 'temperature': -47.54, 'humidity': 4.26}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 20:08:01 WARN Utils: Your hostname, nord-laptop resolves to a loopback address: 127.0.1.1; using 192.168.50.104 instead (on interface wlp0s20f3)\n",
      "24/11/23 20:08:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/nord/.ivy2/cache\n",
      "The jars for the packages stored in: /home/nord/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3327f2c2-e496-4d99-8d21-5c0568c1a33c;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 243ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3327f2c2-e496-4d99-8d21-5c0568c1a33c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/5ms)\n",
      "24/11/23 20:08:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"KafkaSparkStreaming\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set Spark log level to ERROR to reduce logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Define Schema and Read Stream from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for sensor data\n",
    "sensor_schema = StructType([\n",
    "    StructField(\"sensor_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"humidity\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kafka options\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": ','.join(kafka_config['bootstrap_servers']),\n",
    "    \"subscribe\": topic_building_sensors,\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"kafka.security.protocol\": kafka_config['security_protocol'],\n",
    "    \"kafka.sasl.mechanism\": kafka_config['sasl_mechanism'],\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_config[\"username\"]}\" password=\"{kafka_config[\"password\"]}\";'\n",
    "}\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "# Convert value from bytes to string and parse JSON\n",
    "df = df.selectExpr(\"CAST(value AS STRING) as json_string\") \\\n",
    "       .select(F.from_json(F.col(\"json_string\"), sensor_schema).alias(\"data\")) \\\n",
    "       .select(\"data.*\")\n",
    "\n",
    "# Ensure timestamp is in TimestampType\n",
    "df = df.withColumn(\"timestamp\", F.to_timestamp(\"timestamp\"))\n",
    "\n",
    "# Print schema for debugging (can be commented out later)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Load Alert Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+---------------+---------------+----+-------------+\n",
      "| id|humidity_min|humidity_max|temperature_min|temperature_max|code|      message|\n",
      "+---+------------+------------+---------------+---------------+----+-------------+\n",
      "|  1|         0.0|        40.0|           NULL|           NULL| 101| It's too dry|\n",
      "|  2|        60.0|       100.0|           NULL|           NULL| 102| It's too wet|\n",
      "|  3|        NULL|        NULL|         -300.0|           30.0| 103|It's too cold|\n",
      "|  4|        NULL|        NULL|           40.0|          300.0| 104| It's too hot|\n",
      "+---+------------+------------+---------------+---------------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alerts_conditions_df = spark.read.csv(alerts_conditions_file, header=True, inferSchema=True)\n",
    "\n",
    "# Replace -999 with null\n",
    "alerts_conditions_df = alerts_conditions_df.replace(-999, None)\n",
    "\n",
    "# Cast types\n",
    "alerts_conditions_df = alerts_conditions_df.select(\n",
    "    F.col(\"id\").cast(\"int\"),\n",
    "    F.col(\"humidity_min\").cast(\"double\"),\n",
    "    F.col(\"humidity_max\").cast(\"double\"),\n",
    "    F.col(\"temperature_min\").cast(\"double\"),\n",
    "    F.col(\"temperature_max\").cast(\"double\"),\n",
    "    F.col(\"code\").cast(\"string\"),\n",
    "    F.col(\"message\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Show alert conditions\n",
    "alerts_conditions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Compute Averages Using Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_df = df \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(\n",
    "        F.window(F.col(\"timestamp\"), \"1 minute\", \"30 seconds\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        F.avg(\"temperature\").alias(\"t_avg\"),\n",
    "        F.avg(\"humidity\").alias(\"h_avg\")\n",
    "    )\n",
    "\n",
    "# Flatten the window column\n",
    "windowed_df = windowed_df.select(\"window.*\", \"t_avg\", \"h_avg\")\n",
    "\n",
    "# Round average values for readability\n",
    "windowed_df = windowed_df \\\n",
    "    .withColumn(\"t_avg\", F.round(\"t_avg\", 2)) \\\n",
    "    .withColumn(\"h_avg\", F.round(\"h_avg\", 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Cross Join with Alert Conditions and Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = windowed_df.crossJoin(alerts_conditions_df)\n",
    "\n",
    "# Define filter conditions\n",
    "conditions = (\n",
    "    ( (F.col(\"temperature_min\").isNull() | (F.col(\"t_avg\") >= F.col(\"temperature_min\"))) ) &\n",
    "    ( (F.col(\"temperature_max\").isNull() | (F.col(\"t_avg\") <= F.col(\"temperature_max\"))) ) &\n",
    "    ( (F.col(\"humidity_min\").isNull() | (F.col(\"h_avg\") >= F.col(\"humidity_min\"))) ) &\n",
    "    ( (F.col(\"humidity_max\").isNull() | (F.col(\"h_avg\") <= F.col(\"humidity_max\"))) )\n",
    ")\n",
    "\n",
    "# Apply filtering to get alerts\n",
    "alerts_df = joined_df.filter(conditions)\n",
    "\n",
    "# Add current timestamp\n",
    "alerts_df = alerts_df.withColumn(\"timestamp\", F.current_timestamp())\n",
    "\n",
    "# Select necessary columns and structure data with correct field names\n",
    "alerts_df = alerts_df.select(\n",
    "    F.struct(\n",
    "        F.col(\"start\").cast(\"string\").alias(\"start\"),\n",
    "        F.col(\"end\").cast(\"string\").alias(\"end\")\n",
    "    ).alias(\"window\"),\n",
    "    F.round(\"t_avg\", 2).alias(\"t_avg\"),\n",
    "    F.round(\"h_avg\", 2).alias(\"h_avg\"),\n",
    "    \"code\",\n",
    "    \"message\",\n",
    "    F.col(\"timestamp\").cast(\"string\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Write Alerts to Kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 20:08:07 - KafkaLogger - INFO - Started streaming query to write alerts to Kafka topic.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Function to convert DataFrame to JSON string\n",
    "def to_json_string(df):\n",
    "    return df.select(F.to_json(F.struct(\"*\")).alias(\"value\"))\n",
    "\n",
    "kafka_output_options = {\n",
    "    \"kafka.bootstrap.servers\": ','.join(kafka_config['bootstrap_servers']),\n",
    "    \"topic\": topic_alerts,\n",
    "    \"kafka.security.protocol\": kafka_config['security_protocol'],\n",
    "    \"kafka.sasl.mechanism\": kafka_config['sasl_mechanism'],\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_config[\"username\"]}\" password=\"{kafka_config[\"password\"]}\";'\n",
    "}\n",
    "\n",
    "# Write alerts to Kafka topic\n",
    "def start_kafka_query():\n",
    "    kafka_query = to_json_string(alerts_df) \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"kafka\") \\\n",
    "        .options(**kafka_output_options) \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints\") \\\n",
    "        .start()\n",
    "    kafka_query.awaitTermination()\n",
    "\n",
    "logger.info(\"Started streaming query to write alerts to Kafka topic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Log Only Alerts to Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 20:08:07 - KafkaLogger - INFO - Started streaming queries in separate threads.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12 (start_kafka_query):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nord/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_2685103/4221190351.py\", line 22, in start_kafka_query\n",
      "  File \"/home/nord/.local/lib/python3.12/site-packages/pyspark/sql/streaming/query.py\", line 221, in awaitTermination\n",
      "    return self._jsq.awaitTermination()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nord/.local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nord/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = c76379a5-3777-4cc6-a959-a0829e862680, runId = bd13956c-ab9c-4863-8faf-9e083b579487] terminated with exception: Job 91 cancelled because SparkContext was shut down\n",
      "Exception in thread Thread-13 (start_alerts_query):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nord/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_2685103/3970941588.py\", line 16, in start_alerts_query\n",
      "  File \"/home/nord/.local/lib/python3.12/site-packages/pyspark/sql/streaming/query.py\", line 221, in awaitTermination\n",
      "    return self._jsq.awaitTermination()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nord/.local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nord/.local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 6cf6b445-953b-4ad7-8cad-134dd5cdd50a, runId = 627292d6-965e-41b1-b7fd-72e1b83ad395] terminated with exception: org.apache.spark.SparkException: Job 92 cancelled because SparkContext was shut down\n"
     ]
    }
   ],
   "source": [
    "# Function to log only alerts\n",
    "def log_only_alerts(batch_df, batch_id):\n",
    "    print(f\"Processing batch {batch_id}\")\n",
    "    if not batch_df.isEmpty():\n",
    "        print(\"====== ALERTS ======\")\n",
    "        batch_df.show(truncate=False)\n",
    "    else:\n",
    "        print(\"No alerts in this batch.\")\n",
    "\n",
    "\n",
    "# Start streaming query to process alerts and log to console\n",
    "def start_alerts_query():\n",
    "    alerts_query = (\n",
    "        alerts_df.writeStream.outputMode(\"append\").foreachBatch(log_only_alerts).start()\n",
    "    )\n",
    "    alerts_query.awaitTermination()\n",
    "\n",
    "\n",
    "# Start the streaming queries in separate threads\n",
    "def start_streaming_queries():\n",
    "    kafka_thread = threading.Thread(target=start_kafka_query)\n",
    "    alerts_thread = threading.Thread(target=start_alerts_query)\n",
    "    kafka_thread.daemon = True\n",
    "    alerts_thread.daemon = True\n",
    "    kafka_thread.start()\n",
    "    alerts_thread.start()\n",
    "    return kafka_thread, alerts_thread\n",
    "\n",
    "\n",
    "kafka_thread, alerts_thread = start_streaming_queries()\n",
    "logger.info(\"Started streaming queries in separate threads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_alerts():\n",
    "\n",
    "    consumer = KafkaConsumer(\n",
    "        topic_alerts,\n",
    "        bootstrap_servers=kafka_config[\"bootstrap_servers\"],\n",
    "        security_protocol=kafka_config[\"security_protocol\"],\n",
    "        sasl_mechanism=kafka_config[\"sasl_mechanism\"],\n",
    "        sasl_plain_username=kafka_config[\"username\"],\n",
    "        sasl_plain_password=kafka_config[\"password\"],\n",
    "        value_deserializer=lambda m: json.loads(m.decode(\"utf-8\")),\n",
    "        auto_offset_reset=\"latest\",\n",
    "        enable_auto_commit=True,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting Kafka consumer to read alerts...\")\n",
    "    try:\n",
    "        for message in consumer:\n",
    "            logger.info(\"Received alert:\")\n",
    "            logger.info(json.dumps(message.value, indent=2))\n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Kafka consumer stopped.\")\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "# Start the consumer in a separate thread\n",
    "consumer_thread = threading.Thread(target=consume_alerts)\n",
    "consumer_thread.daemon = True  # Set thread as daemon\n",
    "consumer_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-23 20:08:07 - KafkaLogger - INFO - Starting Kafka consumer to read alerts...\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:19 - KafkaLogger - INFO - Received alert:\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:19 - KafkaLogger - INFO - {\n",
      "  \"window\": {\n",
      "    \"start\": \"2024-11-23 20:05:00\",\n",
      "    \"end\": \"2024-11-23 20:06:00\"\n",
      "  },\n",
      "  \"t_avg\": 5.78,\n",
      "  \"h_avg\": 45.83,\n",
      "  \"code\": \"103\",\n",
      "  \"message\": \"It's too cold\",\n",
      "  \"timestamp\": \"2024-11-23 20:08:14.094\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:24 - KafkaLogger - INFO - Received alert:\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:24 - KafkaLogger - INFO - {\n",
      "  \"window\": {\n",
      "    \"start\": \"2024-11-23 20:05:30\",\n",
      "    \"end\": \"2024-11-23 20:06:30\"\n",
      "  },\n",
      "  \"t_avg\": 0.73,\n",
      "  \"h_avg\": 50.41,\n",
      "  \"code\": \"103\",\n",
      "  \"message\": \"It's too cold\",\n",
      "  \"timestamp\": \"2024-11-23 20:08:19.773\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:25 - KafkaLogger - INFO - Received alert:\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:25 - KafkaLogger - INFO - {\n",
      "  \"window\": {\n",
      "    \"start\": \"2024-11-23 20:06:00\",\n",
      "    \"end\": \"2024-11-23 20:07:00\"\n",
      "  },\n",
      "  \"t_avg\": -3.81,\n",
      "  \"h_avg\": 52.98,\n",
      "  \"code\": \"103\",\n",
      "  \"message\": \"It's too cold\",\n",
      "  \"timestamp\": \"2024-11-23 20:08:19.773\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:50 - KafkaLogger - INFO - Received alert:\u001b[0m\n",
      "\u001b[32m2024-11-23 20:08:50 - KafkaLogger - INFO - {\n",
      "  \"window\": {\n",
      "    \"start\": \"2024-11-23 20:07:30\",\n",
      "    \"end\": \"2024-11-23 20:08:30\"\n",
      "  },\n",
      "  \"t_avg\": 4.74,\n",
      "  \"h_avg\": 50.16,\n",
      "  \"code\": \"103\",\n",
      "  \"message\": \"It's too cold\",\n",
      "  \"timestamp\": \"2024-11-23 20:08:50.351\"\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:30 - KafkaLogger - INFO - Received alert:\u001b[0m\n",
      "\u001b[32m2024-11-23 20:09:30 - KafkaLogger - INFO - {\n",
      "  \"window\": {\n",
      "    \"start\": \"2024-11-23 20:08:00\",\n",
      "    \"end\": \"2024-11-23 20:09:00\"\n",
      "  },\n",
      "  \"t_avg\": 6.12,\n",
      "  \"h_avg\": 50.98,\n",
      "  \"code\": \"103\",\n",
      "  \"message\": \"It's too cold\",\n",
      "  \"timestamp\": \"2024-11-23 20:09:23.271\"\n",
      "}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m2024-11-23 20:09:46 - KafkaLogger - WARNING - Streaming processing stopped by user.\u001b[0m\n",
      "24/11/23 20:09:46 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 282, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3b1c4cbb] is aborting.\n",
      "24/11/23 20:09:46 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 282, writer: org.apache.spark.sql.kafka010.KafkaStreamingWrite@3b1c4cbb] aborted.\n",
      "24/11/23 20:09:47 ERROR MicroBatchExecution: Query [id = 6cf6b445-953b-4ad7-8cad-134dd5cdd50a, runId = 627292d6-965e-41b1-b7fd-72e1b83ad395] terminated with error\n",
      "java.util.concurrent.ExecutionException: org.apache.spark.SparkException: Job 92 cancelled because SparkContext was shut down\n",
      "\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:212)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:517)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:208)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:204)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.prepareBroadcast(BroadcastNestedLoopJoinExec.scala:444)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.codegenInner(BroadcastNestedLoopJoinExec.scala:454)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doConsume(BroadcastNestedLoopJoinExec.scala:428)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:196)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:151)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:322)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:538)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doProduce(AggregateCodegenSupport.scala:69)\n",
      "\tat org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doProduce$(AggregateCodegenSupport.scala:65)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doProduce(BroadcastNestedLoopJoinExec.scala:423)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.produce(BroadcastNestedLoopJoinExec.scala:32)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:55)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:42)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:55)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:42)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:660)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:723)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:30)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.SparkException: Job 92 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:455)\n",
      "\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:140)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR MicroBatchExecution: Query [id = c76379a5-3777-4cc6-a959-a0829e862680, runId = bd13956c-ab9c-4863-8faf-9e083b579487] terminated with error\n",
      "org.apache.spark.SparkException: Job 91 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1251)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1251)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3087)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2973)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2258)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@4fe298fd rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@7fd305d0 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 13, active threads = 13, queued tasks = 0, completed tasks = 4930]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@353b155c rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@dade2ca rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 13, active threads = 13, queued tasks = 0, completed tasks = 4930]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@43ba0310 rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@75187606 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 13, active threads = 13, queued tasks = 0, completed tasks = 4930]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@19be57a rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@21aa1455 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 11, active threads = 11, queued tasks = 0, completed tasks = 4932]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@1bcc00ec rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6c47a031 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 11, active threads = 11, queued tasks = 0, completed tasks = 4932]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@7baa320f rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@727fed28 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 10, active threads = 10, queued tasks = 0, completed tasks = 4933]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@30c6fc76 rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@2587a233 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 8, active threads = 8, queued tasks = 0, completed tasks = 4935]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@5c316b8a rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@1bdcd9c8 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 7, active threads = 7, queued tasks = 0, completed tasks = 4936]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@39d5a166 rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@4934eb8b rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 7, active threads = 7, queued tasks = 0, completed tasks = 4936]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@38eb50e9 rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@1b847246 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 7, active threads = 6, queued tasks = 0, completed tasks = 4937]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@a34f511 rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@14a25b09 rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 5, active threads = 5, queued tasks = 0, completed tasks = 4938]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@2d7139b3 rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@378b79ad rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 3, active threads = 3, queued tasks = 0, completed tasks = 4940]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@2a8c82a6 rejected from java.util.concurrent.ThreadPoolExecutor@5d7e2544[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 4927]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:836)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:811)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/11/23 20:09:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@1a8904bb rejected from java.util.concurrent.ThreadPoolExecutor@5e9a1c4[Shutting down, pool size = 3, active threads = 3, queued tasks = 0, completed tasks = 4940]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:363)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\u001b[32m2024-11-23 20:09:47 - KafkaLogger - INFO - Spark session stopped.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0\n",
      "No alerts in this batch.\n",
      "Processing batch 1\n",
      "No alerts in this batch.\n",
      "Processing batch 2\n",
      "No alerts in this batch.\n",
      "Processing batch 3\n",
      "No alerts in this batch.\n",
      "Processing batch 4\n",
      "No alerts in this batch.\n",
      "Processing batch 5\n",
      "No alerts in this batch.\n",
      "Processing batch 6\n",
      "====== ALERTS ======\n",
      "+------------------------------------------+-----+-----+----+-------------+-----------------------+\n",
      "|window                                    |t_avg|h_avg|code|message      |timestamp              |\n",
      "+------------------------------------------+-----+-----+----+-------------+-----------------------+\n",
      "|{2024-11-23 20:07:30, 2024-11-23 20:08:30}|12.68|49.24|103 |It's too cold|2024-11-23 20:08:57.321|\n",
      "+------------------------------------------+-----+-----+----+-------------+-----------------------+\n",
      "\n",
      "Processing batch 7\n",
      "No alerts in this batch.\n",
      "Processing batch 8\n",
      "No alerts in this batch.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning(\"Streaming processing stopped by user.\")\n",
    "    spark.stop()\n",
    "    logger.info(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
